{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dcbb154",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da5e5e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sophiasarica/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sophiasarica/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sophiasarica/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, download\n",
    "download(['punkt', 'averaged_perceptron_tagger', 'stopwords'])\n",
    "\n",
    "def get_tokens(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return words\n",
    "\n",
    "sentence = \"This is an example sentence.\"\n",
    "print(get_tokens(sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a5d4c",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ac54cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('sentence', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Define the function for tokenizing sentences\n",
    "def get_tokens(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return words\n",
    "\n",
    "# Define the function for getting part-of-speech tags\n",
    "def get_pos(words):\n",
    "    return pos_tag(words)\n",
    "\n",
    "# Define a sentence to process\n",
    "sentence = \"This is an example sentence.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "words = get_tokens(sentence)\n",
    "\n",
    "# Apply POS tagging\n",
    "pos_tags = get_pos(words)\n",
    "\n",
    "# Print the part-of-speech tags\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b69ae84",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b3882b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   'i',\n",
      "    'me',\n",
      "    'my',\n",
      "    'myself',\n",
      "    'we',\n",
      "    'our',\n",
      "    'ours',\n",
      "    'ourselves',\n",
      "    'you',\n",
      "    \"you're\",\n",
      "    \"you've\",\n",
      "    \"you'll\",\n",
      "    \"you'd\",\n",
      "    'your',\n",
      "    'yours',\n",
      "    'yourself',\n",
      "    'yourselves',\n",
      "    'he',\n",
      "    'him',\n",
      "    'his',\n",
      "    'himself',\n",
      "    'she',\n",
      "    \"she's\",\n",
      "    'her',\n",
      "    'hers',\n",
      "    'herself',\n",
      "    'it',\n",
      "    \"it's\",\n",
      "    'its',\n",
      "    'itself',\n",
      "    'they',\n",
      "    'them',\n",
      "    'their',\n",
      "    'theirs',\n",
      "    'themselves',\n",
      "    'what',\n",
      "    'which',\n",
      "    'who',\n",
      "    'whom',\n",
      "    'this',\n",
      "    'that',\n",
      "    \"that'll\",\n",
      "    'these',\n",
      "    'those',\n",
      "    'am',\n",
      "    'is',\n",
      "    'are',\n",
      "    'was',\n",
      "    'were',\n",
      "    'be',\n",
      "    'been',\n",
      "    'being',\n",
      "    'have',\n",
      "    'has',\n",
      "    'had',\n",
      "    'having',\n",
      "    'do',\n",
      "    'does',\n",
      "    'did',\n",
      "    'doing',\n",
      "    'a',\n",
      "    'an',\n",
      "    'the',\n",
      "    'and',\n",
      "    'but',\n",
      "    'if',\n",
      "    'or',\n",
      "    'because',\n",
      "    'as',\n",
      "    'until',\n",
      "    'while',\n",
      "    'of',\n",
      "    'at',\n",
      "    'by',\n",
      "    'for',\n",
      "    'with',\n",
      "    'about',\n",
      "    'against',\n",
      "    'between',\n",
      "    'into',\n",
      "    'through',\n",
      "    'during',\n",
      "    'before',\n",
      "    'after',\n",
      "    'above',\n",
      "    'below',\n",
      "    'to',\n",
      "    'from',\n",
      "    'up',\n",
      "    'down',\n",
      "    'in',\n",
      "    'out',\n",
      "    'on',\n",
      "    'off',\n",
      "    'over',\n",
      "    'under',\n",
      "    'again',\n",
      "    'further',\n",
      "    'then',\n",
      "    'once',\n",
      "    'here',\n",
      "    'there',\n",
      "    'when',\n",
      "    'where',\n",
      "    'why',\n",
      "    'how',\n",
      "    'all',\n",
      "    'any',\n",
      "    'both',\n",
      "    'each',\n",
      "    'few',\n",
      "    'more',\n",
      "    'most',\n",
      "    'other',\n",
      "    'some',\n",
      "    'such',\n",
      "    'no',\n",
      "    'nor',\n",
      "    'not',\n",
      "    'only',\n",
      "    'own',\n",
      "    'same',\n",
      "    'so',\n",
      "    'than',\n",
      "    'too',\n",
      "    'very',\n",
      "    's',\n",
      "    't',\n",
      "    'can',\n",
      "    'will',\n",
      "    'just',\n",
      "    'don',\n",
      "    \"don't\",\n",
      "    'should',\n",
      "    \"should've\",\n",
      "    'now',\n",
      "    'd',\n",
      "    'll',\n",
      "    'm',\n",
      "    'o',\n",
      "    're',\n",
      "    've',\n",
      "    'y',\n",
      "    'ain',\n",
      "    'aren',\n",
      "    \"aren't\",\n",
      "    'couldn',\n",
      "    \"couldn't\",\n",
      "    'didn',\n",
      "    \"didn't\",\n",
      "    'doesn',\n",
      "    \"doesn't\",\n",
      "    'hadn',\n",
      "    \"hadn't\",\n",
      "    'hasn',\n",
      "    \"hasn't\",\n",
      "    'haven',\n",
      "    \"haven't\",\n",
      "    'isn',\n",
      "    \"isn't\",\n",
      "    'ma',\n",
      "    'mightn',\n",
      "    \"mightn't\",\n",
      "    'mustn',\n",
      "    \"mustn't\",\n",
      "    'needn',\n",
      "    \"needn't\",\n",
      "    'shan',\n",
      "    \"shan't\",\n",
      "    'shouldn',\n",
      "    \"shouldn't\",\n",
      "    'wasn',\n",
      "    \"wasn't\",\n",
      "    'weren',\n",
      "    \"weren't\",\n",
      "    'won',\n",
      "    \"won't\",\n",
      "    'wouldn',\n",
      "    \"wouldn't\"]\n",
      "['This', 'is', 'an', 'example', 'sentence', '.']\n",
      "example sentence .\n",
      "example sentence .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sophiasarica/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pprint\n",
    "\n",
    "# Initialize stopwords and pretty printer\n",
    "stop_words = stopwords.words('english')\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(stop_words)\n",
    "\n",
    "# Define a sentence to process\n",
    "sentence = \"This is an example sentence.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "sentence_words = word_tokenize(sentence)\n",
    "print(sentence_words)\n",
    "\n",
    "# Define the function to remove stopwords\n",
    "def remove_stop_words(words, stop_words):\n",
    "    return ' '.join([word for word in words if word.lower() not in stop_words])\n",
    "\n",
    "# Remove stopwords and print the result\n",
    "filtered_sentence = remove_stop_words(sentence_words, stop_words)\n",
    "print(filtered_sentence)\n",
    "\n",
    "# Extend the stopwords list\n",
    "stop_words.extend(['as'])\n",
    "\n",
    "# Remove stopwords again with the extended list and print the result\n",
    "filtered_sentence_extended = remove_stop_words(sentence_words, stop_words)\n",
    "print(filtered_sentence_extended)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd3205a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9298e20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e4a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
