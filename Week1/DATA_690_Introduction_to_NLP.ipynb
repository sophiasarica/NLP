{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Vrj9OgbZnx3y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When Tim Cook took over as chief executive of Apple, it was a corporate transition unlike any other.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'When Tim Cook took over as chief executive of Apple, it was a corporate transition unlike any other.'\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udHh55j0qP4n"
   },
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3YEJOOrupi0M"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sophiasarica/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sophiasarica/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'Tim', 'Cook', 'took', 'over', 'as', 'chief', 'executive', 'of', 'Apple', ',', 'it', 'was', 'a', 'corporate', 'transition', 'unlike', 'any', 'other', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sophiasarica/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize,download\n",
    "download(['punkt','averaged_perceptron_tagger','stopwords'])\n",
    "def get_tokens(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return words\n",
    "print(get_tokens(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1Alef0GqStf"
   },
   "source": [
    "POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PspRUQhLqkd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'Tim', 'Cook', 'took', 'over', 'as', 'chief', 'executive', 'of', 'Apple', ',', 'it', 'was', 'a', 'corporate', 'transition', 'unlike', 'any', 'other', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('When', 'WRB'),\n",
       " ('Tim', 'NNP'),\n",
       " ('Cook', 'NNP'),\n",
       " ('took', 'VBD'),\n",
       " ('over', 'RP'),\n",
       " ('as', 'IN'),\n",
       " ('chief', 'JJ'),\n",
       " ('executive', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Apple', 'NNP'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('corporate', 'JJ'),\n",
       " ('transition', 'NN'),\n",
       " ('unlike', 'IN'),\n",
       " ('any', 'DT'),\n",
       " ('other', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "def get_tokens(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    return words\n",
    "words  = get_tokens(sentence)\n",
    "print(words)\n",
    "\n",
    "def get_pos(words):\n",
    "    return pos_tag(words)\n",
    "get_pos(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9t7zqSiq6jw"
   },
   "source": [
    "Tokenize and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GzJkt2CXrBb6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sophiasarica/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Jz0k_9JFrDZD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   'i',\n",
      "    'me',\n",
      "    'my',\n",
      "    'myself',\n",
      "    'we',\n",
      "    'our',\n",
      "    'ours',\n",
      "    'ourselves',\n",
      "    'you',\n",
      "    \"you're\",\n",
      "    \"you've\",\n",
      "    \"you'll\",\n",
      "    \"you'd\",\n",
      "    'your',\n",
      "    'yours',\n",
      "    'yourself',\n",
      "    'yourselves',\n",
      "    'he',\n",
      "    'him',\n",
      "    'his',\n",
      "    'himself',\n",
      "    'she',\n",
      "    \"she's\",\n",
      "    'her',\n",
      "    'hers',\n",
      "    'herself',\n",
      "    'it',\n",
      "    \"it's\",\n",
      "    'its',\n",
      "    'itself',\n",
      "    'they',\n",
      "    'them',\n",
      "    'their',\n",
      "    'theirs',\n",
      "    'themselves',\n",
      "    'what',\n",
      "    'which',\n",
      "    'who',\n",
      "    'whom',\n",
      "    'this',\n",
      "    'that',\n",
      "    \"that'll\",\n",
      "    'these',\n",
      "    'those',\n",
      "    'am',\n",
      "    'is',\n",
      "    'are',\n",
      "    'was',\n",
      "    'were',\n",
      "    'be',\n",
      "    'been',\n",
      "    'being',\n",
      "    'have',\n",
      "    'has',\n",
      "    'had',\n",
      "    'having',\n",
      "    'do',\n",
      "    'does',\n",
      "    'did',\n",
      "    'doing',\n",
      "    'a',\n",
      "    'an',\n",
      "    'the',\n",
      "    'and',\n",
      "    'but',\n",
      "    'if',\n",
      "    'or',\n",
      "    'because',\n",
      "    'as',\n",
      "    'until',\n",
      "    'while',\n",
      "    'of',\n",
      "    'at',\n",
      "    'by',\n",
      "    'for',\n",
      "    'with',\n",
      "    'about',\n",
      "    'against',\n",
      "    'between',\n",
      "    'into',\n",
      "    'through',\n",
      "    'during',\n",
      "    'before',\n",
      "    'after',\n",
      "    'above',\n",
      "    'below',\n",
      "    'to',\n",
      "    'from',\n",
      "    'up',\n",
      "    'down',\n",
      "    'in',\n",
      "    'out',\n",
      "    'on',\n",
      "    'off',\n",
      "    'over',\n",
      "    'under',\n",
      "    'again',\n",
      "    'further',\n",
      "    'then',\n",
      "    'once',\n",
      "    'here',\n",
      "    'there',\n",
      "    'when',\n",
      "    'where',\n",
      "    'why',\n",
      "    'how',\n",
      "    'all',\n",
      "    'any',\n",
      "    'both',\n",
      "    'each',\n",
      "    'few',\n",
      "    'more',\n",
      "    'most',\n",
      "    'other',\n",
      "    'some',\n",
      "    'such',\n",
      "    'no',\n",
      "    'nor',\n",
      "    'not',\n",
      "    'only',\n",
      "    'own',\n",
      "    'same',\n",
      "    'so',\n",
      "    'than',\n",
      "    'too',\n",
      "    'very',\n",
      "    's',\n",
      "    't',\n",
      "    'can',\n",
      "    'will',\n",
      "    'just',\n",
      "    'don',\n",
      "    \"don't\",\n",
      "    'should',\n",
      "    \"should've\",\n",
      "    'now',\n",
      "    'd',\n",
      "    'll',\n",
      "    'm',\n",
      "    'o',\n",
      "    're',\n",
      "    've',\n",
      "    'y',\n",
      "    'ain',\n",
      "    'aren',\n",
      "    \"aren't\",\n",
      "    'couldn',\n",
      "    \"couldn't\",\n",
      "    'didn',\n",
      "    \"didn't\",\n",
      "    'doesn',\n",
      "    \"doesn't\",\n",
      "    'hadn',\n",
      "    \"hadn't\",\n",
      "    'hasn',\n",
      "    \"hasn't\",\n",
      "    'haven',\n",
      "    \"haven't\",\n",
      "    'isn',\n",
      "    \"isn't\",\n",
      "    'ma',\n",
      "    'mightn',\n",
      "    \"mightn't\",\n",
      "    'mustn',\n",
      "    \"mustn't\",\n",
      "    'needn',\n",
      "    \"needn't\",\n",
      "    'shan',\n",
      "    \"shan't\",\n",
      "    'shouldn',\n",
      "    \"shouldn't\",\n",
      "    'wasn',\n",
      "    \"wasn't\",\n",
      "    'weren',\n",
      "    \"weren't\",\n",
      "    'won',\n",
      "    \"won't\",\n",
      "    'wouldn',\n",
      "    \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "stop_words = stopwords.words('english')\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wSCrgOiVr_Ky"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'Tim', 'Cook', 'took', 'over', 'as', 'chief', 'executive', 'of', 'Apple', ',', 'it', 'was', 'a', 'corporate', 'transition', 'unlike', 'any', 'other', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence_words = word_tokenize(sentence)\n",
    "print(sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yfBwdxhtsNHw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Tim Cook took chief executive Apple , corporate transition unlike .\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_words(sentence, stop_words):\n",
    "    return ' '.join([word for word in sentence if word not in stop_words])\n",
    "    print(remove_stop_words(sentence_words, stop_words))\n",
    "stop_words.extend(['as'])\n",
    "print(remove_stop_words(sentence_words,stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jInI755Os8M1"
   },
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdDHjGB6s8V-"
   },
   "outputs": [],
   "source": [
    "sentence= 'When Tim Cook took over in 11 as chief executive of Apple, it was a corporate transition unlike any other in the US.'\n",
    "def normalize(text):\n",
    "    return text.replace(\"US\", \"United States\").replace(\"11\",\n",
    "        \"2011\")\n",
    "normalized_sentence = normalize(sentence)\n",
    "print(normalized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1kSzOxquRgI"
   },
   "source": [
    "Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqKKvpjhuikl"
   },
   "outputs": [],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IV66RNAnuW-1"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "spell('Natureal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C4tu0ClFuwNt"
   },
   "outputs": [],
   "source": [
    "sentence = word_tokenize(\"Ntural Luanguage Processin deals with the art of extracting insightes from Natural Languaes\")\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Ls7KX05u2sY"
   },
   "outputs": [],
   "source": [
    "def correct_spelling(tokens):\n",
    "    sentence_corrected = ' '.join([spell(word) for word in tokens])\n",
    "    return sentence_corrected\n",
    "#print(correct_spelling(sentence))\n",
    "print(['Natural', 'Language', 'Procession', 'deals', 'with', 'the', 'art', 'of', 'extracting', 'insights', 'from', 'Natural', 'Languages'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6QvF91nvLtd"
   },
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZOzoN8KvKtT"
   },
   "outputs": [],
   "source": [
    "from nltk import stem\n",
    "def get_stems(word,stemmer):\n",
    "    return stemmer.stem(word)\n",
    "porterStem = stem.PorterStemmer()\n",
    "\n",
    "get_stems(\"production\",porterStem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmDXmF66vdqB"
   },
   "outputs": [],
   "source": [
    "stemmer = stem.SnowballStemmer(\"english\")\n",
    "get_stems(\"battling\",stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9bCSPlhvr04"
   },
   "source": [
    "Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhnjUVmMvstd"
   },
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_lemma(word):\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "get_lemma('coming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pS7J70HGwAtM"
   },
   "outputs": [],
   "source": [
    "get_lemma('production')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEBIOvuKwbYB"
   },
   "source": [
    "Chunking/ Named Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chdB5St5wbhX"
   },
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk import word_tokenize\n",
    "download('maxent_ne_chunker')\n",
    "download('words')\n",
    "sentence = \"We are reading a book published by Packt which is based out of Birmingham.\"\n",
    "def get_ner(text):\n",
    "    i = ne_chunk(pos_tag(word_tokenize(text)), binary=True)\n",
    "    return [a for a in i if len(a)==1]\n",
    "get_ner(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUIFYzwMxEZB"
   },
   "source": [
    "Word Disembiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ai1aq11TxDXb"
   },
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence1 = \"Keep your savings in the bank\"\n",
    "sentence2 = \"It's so risky to drive over the banks of the road\"\n",
    "\n",
    "def get_synset(sentence, word):\n",
    "    return lesk(word_tokenize(sentence), word)\n",
    "get_synset(sentence1,'bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOBZrznrx1Ha"
   },
   "outputs": [],
   "source": [
    "get_synset(sentence2,'bank')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
